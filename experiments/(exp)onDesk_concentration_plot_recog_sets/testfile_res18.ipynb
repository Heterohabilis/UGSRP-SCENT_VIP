{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Use pretrained ResNet cnn to predict concentration plot over 6-minute sampling\n",
    "### Firstly, import dependencies"
   ],
   "id": "3efbaa87fbf7c4e8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T17:04:27.823788Z",
     "start_time": "2024-08-01T17:04:26.042922Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "cudnn.benchmark = True\n",
    "plt.ion()   # interactive mode"
   ],
   "id": "1e9608c34e204461",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<contextlib.ExitStack at 0x793f5b13a9d0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Then, we import the pre-fine-tuned model (a .pth file), and pass it to gpu memory.",
   "id": "60ab05c460807568"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T17:04:32.107906Z",
     "start_time": "2024-08-01T17:04:29.827527Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = torchvision.models.resnet18(pretrained=True)            # initialize the model\n",
    "num_ftrs = model.fc.in_features                                 # get the number of features\n",
    "model.fc = nn.Linear(num_ftrs, 2)                               # setting up the final layer of the neural network to take num_ftrs inputs and produce 2 outputs (experiment, may be more classes)\n",
    "model.load_state_dict(torch.load(\"concen_identifier.pth\"))      # load the pre-trained model\n",
    "model.cuda()                                                    # pass the model to cuda\n",
    "model.eval()                                                    # test the model-loading condition"
   ],
   "id": "ef9abf7f7c020ce6",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cybercricetus/anaconda3/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/cybercricetus/anaconda3/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### We define some pre-process transformation for the test images.",
   "id": "1be602f50f13c88c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T17:04:33.706256Z",
     "start_time": "2024-08-01T17:04:33.703102Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize((224, 224), transforms.InterpolationMode.BILINEAR),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize((224,224), transforms.InterpolationMode.BILINEAR),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}           # ResNet only support 224*224 images, resize to avoid losing information"
   ],
   "id": "d7b97f29ac235660",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Define device variable (optional)",
   "id": "1130a006a60a8585"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T17:04:34.467534Z",
     "start_time": "2024-08-01T17:04:34.465300Z"
    }
   },
   "cell_type": "code",
   "source": "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")",
   "id": "fdd6e005b20f6a3e",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Define directory and classes",
   "id": "37638877f201a17f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T17:04:38.567675Z",
     "start_time": "2024-08-01T17:04:38.565731Z"
    }
   },
   "cell_type": "code",
   "source": [
    "_dir = \"tests\"\n",
    "_classes = ['near', 'far']\n",
    "pic_name_list = _1st_dir = os.listdir(_dir)"
   ],
   "id": "205ed93390c34d0e",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Traverse through the directory, read the images to CPU buffer and then GPU buffer, and finally do prediction with model loaded in GPU memory",
   "id": "cd058d4064a32647"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T17:04:39.766268Z",
     "start_time": "2024-08-01T17:04:39.215827Z"
    }
   },
   "cell_type": "code",
   "source": [
    "count = 0                                               # correct counts\n",
    "result_cmp_set = {}                                     # actual:pred\n",
    "for name in pic_name_list:                              # traverse the directory\n",
    "    img = Image.open(_dir+'/'+name).convert('RGB')      # open the image\n",
    "    img = data_transforms['val'](img)                   # do the transformation\n",
    "    img = img.unsqueeze(0)                      \n",
    "    img = img.to(device)                                # pass to GPU memory\n",
    "\n",
    "    with torch.no_grad():           \n",
    "        outputs = model(img)                            # predict\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "    result_cmp_set[name] = _classes[preds]              # add prediction to the collection\n",
    "    if _classes[preds][0] == name[1]:                   # if correctly predicted, count+1\n",
    "        count += 1"
   ],
   "id": "fbf7672aa4885314",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Check the result ",
   "id": "3b351f506d7e730"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T17:04:40.444405Z",
     "start_time": "2024-08-01T17:04:40.442058Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(result_cmp_set)                                 \n",
    "print('accuracy is: ', count / len(pic_name_list))   # accuracy\n",
    "print(\"Correct count = \", count)\n",
    "print(\"Incorrect count = \", len(pic_name_list)-count)"
   ],
   "id": "13532040201f5240",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_far6.png': 'far', '_near3.png': 'near', '_far5.png': 'far', '_far2.png': 'far', '_far8.png': 'far', '_far3.png': 'far', '_near6.png': 'near', '_far9.png': 'far', '_near9.png': 'near', '_near7.png': 'near', '_far10.png': 'far', '_far7.png': 'far', '_near2.png': 'near', '_far4.png': 'far', '_near4.png': 'far', '_near5.png': 'near', '_near8.png': 'near', '_far1.png': 'far', '_near1.png': 'near', '_near10.png': 'near'}\n",
      "accuracy is:  0.95\n",
      "Correct count =  19\n",
      "Incorrect count =  1\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "71eb361f18b5aeda"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
